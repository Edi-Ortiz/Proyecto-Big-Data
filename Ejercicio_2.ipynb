{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5c22ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71267f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/09 00:23:46 WARN Utils: Your hostname, ortiz-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "21/12/09 00:23:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ortiz/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/09 00:23:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('lrexample').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf6be358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+------+--------+-----+\n",
      "|number|       day|season| yr|mnth| hr|holiday|weekday|workingday|weathersit|temp| atemp| hum|windspeed|casual|register|total|\n",
      "+------+----------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+------+--------+-----+\n",
      "|     1|2011-01-01|     1|  0|   1|  0|      0|      6|         0|         1|0.24|0.2879|0.81|      0.0|     3|      13|   16|\n",
      "|     2|2011-01-01|     1|  0|   1|  1|      0|      6|         0|         1|0.22|0.2727| 0.8|      0.0|     8|      32|   40|\n",
      "|     3|2011-01-01|     1|  0|   1|  2|      0|      6|         0|         1|0.22|0.2727| 0.8|      0.0|     5|      27|   32|\n",
      "|     4|2011-01-01|     1|  0|   1|  3|      0|      6|         0|         1|0.24|0.2879|0.75|      0.0|     3|      10|   13|\n",
      "|     5|2011-01-01|     1|  0|   1|  4|      0|      6|         0|         1|0.24|0.2879|0.75|      0.0|     0|       1|    1|\n",
      "|     6|2011-01-01|     1|  0|   1|  5|      0|      6|         0|         2|0.24|0.2576|0.75|   0.0896|     0|       1|    1|\n",
      "|     7|2011-01-01|     1|  0|   1|  6|      0|      6|         0|         1|0.22|0.2727| 0.8|      0.0|     2|       0|    2|\n",
      "|     8|2011-01-01|     1|  0|   1|  7|      0|      6|         0|         1| 0.2|0.2576|0.86|      0.0|     1|       2|    3|\n",
      "|     9|2011-01-01|     1|  0|   1|  8|      0|      6|         0|         1|0.24|0.2879|0.75|      0.0|     1|       7|    8|\n",
      "|    10|2011-01-01|     1|  0|   1|  9|      0|      6|         0|         1|0.32|0.3485|0.76|      0.0|     8|       6|   14|\n",
      "|    11|2011-01-01|     1|  0|   1| 10|      0|      6|         0|         1|0.38|0.3939|0.76|   0.2537|    12|      24|   36|\n",
      "|    12|2011-01-01|     1|  0|   1| 11|      0|      6|         0|         1|0.36|0.3333|0.81|   0.2836|    26|      30|   56|\n",
      "|    13|2011-01-01|     1|  0|   1| 12|      0|      6|         0|         1|0.42|0.4242|0.77|   0.2836|    29|      55|   84|\n",
      "|    14|2011-01-01|     1|  0|   1| 13|      0|      6|         0|         2|0.46|0.4545|0.72|   0.2985|    47|      47|   94|\n",
      "|    15|2011-01-01|     1|  0|   1| 14|      0|      6|         0|         2|0.46|0.4545|0.72|   0.2836|    35|      71|  106|\n",
      "|    16|2011-01-01|     1|  0|   1| 15|      0|      6|         0|         2|0.44|0.4394|0.77|   0.2985|    40|      70|  110|\n",
      "|    17|2011-01-01|     1|  0|   1| 16|      0|      6|         0|         2|0.42|0.4242|0.82|   0.2985|    41|      52|   93|\n",
      "|    18|2011-01-01|     1|  0|   1| 17|      0|      6|         0|         2|0.44|0.4394|0.82|   0.2836|    15|      52|   67|\n",
      "|    19|2011-01-01|     1|  0|   1| 18|      0|      6|         0|         3|0.42|0.4242|0.88|   0.2537|     9|      26|   35|\n",
      "|    20|2011-01-01|     1|  0|   1| 19|      0|      6|         0|         3|0.42|0.4242|0.88|   0.2537|     6|      31|   37|\n",
      "+------+----------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"carDetail.csv\", header=\"true\", inferSchema=\"true\")\n",
    "df.cache()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf678b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- season: integer (nullable = true)\n",
      " |-- yr: integer (nullable = true)\n",
      " |-- mnth: integer (nullable = true)\n",
      " |-- hr: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- workingday: integer (nullable = true)\n",
      " |-- weathersit: integer (nullable = true)\n",
      " |-- temp: integer (nullable = true)\n",
      " |-- windspeed: integer (nullable = true)\n",
      " |-- total: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(\"number\").drop(\"day\").drop(\"casual\").drop(\"register\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "285e4abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- season: integer (nullable = true)\n",
      " |-- yr: integer (nullable = true)\n",
      " |-- mnth: integer (nullable = true)\n",
      " |-- hr: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- workingday: integer (nullable = true)\n",
      " |-- weathersit: integer (nullable = true)\n",
      " |-- temp: integer (nullable = true)\n",
      " |-- windspeed: integer (nullable = true)\n",
      " |-- total: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col  \n",
    "df = df.select([col(c).cast(\"int\").alias(c) for c in df.columns])\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e300e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 7761 training examples and 3237 test examples.\n"
     ]
    }
   ],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3])\n",
    "print(\"We have %d training examples and %d test examples.\" % (train.count(), test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e1302ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
    "featuresCols = df.columns\n",
    "featuresCols.remove('total')\n",
    "vectorAssembler = VectorAssembler(inputCols=featuresCols, outputCol=\"rawFeatures\")\n",
    "vectorIndexer = VectorIndexer(inputCol=\"rawFeatures\", outputCol=\"features\", maxCategories=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b0af77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbt = GBTRegressor(labelCol=\"total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2745551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "  .addGrid(gbt.maxDepth, [2, 5])\\\n",
    "  .addGrid(gbt.maxIter, [10, 100])\\\n",
    "  .build()\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=gbt.getLabelCol(), predictionCol=gbt.getPredictionCol())\n",
    "cv = CrossValidator(estimator=gbt, evaluator=evaluator, estimatorParamMaps=paramGrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "198bf942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[vectorAssembler, vectorIndexer, cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff25f0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipelineModel = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "503197bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+------+---+----+---+-------+----------+----------+----+---------+\n",
      "|total|         prediction|season| yr|mnth| hr|weekday|workingday|weathersit|temp|windspeed|\n",
      "+-----+-------------------+------+---+----+---+-------+----------+----------+----+---------+\n",
      "|   22| 23.076537025007802|     1|  0|   1|  0|      0|         0|         1|   0|        0|\n",
      "|   39| 23.076537025007802|     1|  0|   1|  0|      0|         0|         1|   0|        0|\n",
      "|   17| 24.982277968497186|     1|  0|   1|  0|      1|         0|         2|   0|        0|\n",
      "|    5|  7.145147306133547|     1|  0|   1|  0|      2|         1|         1|   0|        0|\n",
      "|   11|  7.088469398020875|     1|  0|   1|  0|      4|         1|         1|   0|        0|\n",
      "|   13|  7.088469398020875|     1|  0|   1|  0|      4|         1|         1|   0|        0|\n",
      "|    9| 24.288630300128073|     1|  0|   1|  0|      5|         1|         2|   0|        0|\n",
      "|   17| 24.288630300128073|     1|  0|   1|  0|      5|         1|         2|   0|        0|\n",
      "|   21| 24.288630300128073|     1|  0|   1|  0|      5|         1|         2|   0|        0|\n",
      "|   28| 28.699235199496584|     1|  0|   1|  0|      6|         0|         1|   0|        0|\n",
      "|   23| 24.871809096823768|     1|  0|   1|  1|      0|         0|         1|   0|        0|\n",
      "|    6|-1.9604887445878032|     1|  0|   1|  1|      3|         1|         2|   0|        0|\n",
      "|    7|  -6.84398616302957|     1|  0|   1|  1|      3|         1|         3|   0|        0|\n",
      "|    5|-0.8664032713233228|     1|  0|   1|  1|      4|         1|         1|   0|        0|\n",
      "|    5| 2.0579834373556936|     1|  0|   1|  1|      5|         1|         1|   0|        0|\n",
      "|   40| 29.747267557935018|     1|  0|   1|  1|      6|         0|         1|   0|        0|\n",
      "|   16|  21.51908931553533|     1|  0|   1|  1|      6|         0|         2|   0|        0|\n",
      "|   11|   26.6434435376645|     1|  0|   1|  2|      0|         0|         1|   0|        0|\n",
      "|    2| -5.018357487823418|     1|  0|   1|  2|      3|         1|         1|   0|        0|\n",
      "|    2|-4.9376956698090035|     1|  0|   1|  2|      4|         1|         1|   0|        0|\n",
      "+-----+-------------------+------+---+----+---+-------+----------+----------+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = pipelineModel.transform(test)\n",
    "(predictions.select(\"total\", \"prediction\", *featuresCols)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6228e043",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5709:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on our test set: 43.4065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"RMSE on our test set: %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8488411c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
